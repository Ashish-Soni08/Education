{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5: Compare the performance of smaller distilled multilingual models as compared to their largest counterparts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6: Evaluate Different distance functions used to measure Semantic Similarity in a practical setting**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Methods (Traditional)\n",
    "\n",
    "1. Jaccard Similarity - refers to the number of words that are common in two sentences of data, over the total number of words in the two sentences.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"I am Ashish, I love playing football. I wanted to be a professional football player.\"\n",
    "second_sentence = \"I am Ahmed, I wish I could play football. My dream was to be a professional football player.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentences into list of words\n",
    "first_sentence = first_sentence.split()\n",
    "second_sentence = second_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'football.', 'a', 'to', 'Ashish,', 'player.', 'love', 'wanted', 'be', 'professional', 'playing', 'football', 'I', 'am'}\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'football.', 'to', 'My', 'could', 'am', 'play', 'a', 'player.', 'dream', 'be', 'professional', 'Ahmed,', 'was', 'football', 'I', 'wish'}\n"
     ]
    }
   ],
   "source": [
    "# Convert our list of words into a sets(to remove duplicates)\n",
    "first_sentence = set(first_sentence)\n",
    "second_sentence = set(second_sentence)\n",
    "\n",
    "print(first_sentence)\n",
    "print('----' * 34)\n",
    "print(second_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'football.', 'a', 'to', 'player.', 'be', 'professional', 'football', 'I', 'am'}\n",
      "--------------------------------------------------------------------------------\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Calculate the shared words between the two sentences\n",
    "shared_words = first_sentence.intersection(second_sentence)\n",
    "print(shared_words)\n",
    "print('----' * 20)\n",
    "print(len(shared_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'football.', 'was', 'to', 'Ashish,', 'love', 'My', 'could', 'am', 'play', 'a', 'player.', 'dream', 'wanted', 'be', 'professional', 'Ahmed,', 'playing', 'football', 'I', 'wish'}\n",
      "--------------------------------------------------------------------------------\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of unique words in both sentences\n",
    "total_words = first_sentence.union(second_sentence)\n",
    "print(total_words)\n",
    "print('----' * 20)\n",
    "print(len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n"
     ]
    }
   ],
   "source": [
    "jacard_similarity = len(shared_words) / len(total_words)\n",
    "print(jacard_similarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges:\n",
    "\n",
    "Two sentences that share nothing but words like 'the', 'a', 'how' etc will have a high similarity score despite being completely dissimilar.\n",
    "\n",
    "Solution:\n",
    "\n",
    "We can use stopword removal, stemmning or lemmatization (so words like 'Travelling and 'travels' can match) and other preprocessing techniques. \n",
    "\n",
    "However, this will not work for languages like Chinese, Japanese, Korean etc. where there is no concept of a word. and There are methods that avoid these problems altogether.\n",
    "Also, for our use case we need to keep stop words for lexical search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w-Shingling\n",
    "\n",
    "w-shingling is a similar method to Jaccard similarity, but instead of using words, we use n-grams (sequences of n words).\n",
    "Lets see an example: using bigrams (n=2) e.g 2-shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentences into list of words\n",
    "first_sentence = first_sentence.split()\n",
    "second_sentence = second_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ashish, I', 'I am', 'I love', 'am Ashish,', 'be a', 'football. I', 'a professional', 'love playing', 'professional football', 'to be', 'wanted to', 'I wanted', 'playing football.', 'football player.'}\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'I wish', 'I could', 'I am', 'was to', 'could play', 'My dream', 'be a', 'professional football', 'a professional', 'dream was', 'football. My', 'play football.', 'to be', 'Ahmed, I', 'wish I', 'football player.', 'am Ahmed,'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence_shingle = set([' '.join([first_sentence[i], first_sentence[i+1]]) for i in range(len(first_sentence)) if i != len(first_sentence) - 1])\n",
    "second_sentence_shingle = set([' '.join([second_sentence[i], second_sentence[i+1]]) for i in range(len(second_sentence)) if i != len(second_sentence) - 1])\n",
    "\n",
    "print(first_sentence_shingle)\n",
    "print('----' * 34)  \n",
    "print(second_sentence_shingle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I am',\n",
       " 'a professional',\n",
       " 'be a',\n",
       " 'football player.',\n",
       " 'professional football',\n",
       " 'to be'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_sentence_shingle.intersection(first_sentence_shingle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac(x: set, y: set):\n",
    "    shared = x.intersection(y)  # selects shared tokens only\n",
    "    return len(shared) / len(x.union(y))  # union adds both sets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac(first_sentence_shingle, second_sentence_shingle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein Distance\n",
    "\n",
    "\n",
    "We will be using a Wagner-Fischer matrix to calculate our Levenshtein distance, let's write a function that will perform this operation for us given two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def leven(a, b):\n",
    "    # we must add an additional character at the start of each string\n",
    "    a = f' {a}'\n",
    "    b = f' {b}'\n",
    "    # initialize empty zero array\n",
    "    lev = np.zeros((len(a), len(b)))\n",
    "    # now begin iterating through each value, finding the best path\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            if min([i, j]) == 0:\n",
    "                lev[i, j] = max([i, j])\n",
    "            else:\n",
    "                # calculate our three possible operations\n",
    "                x = lev[i-1, j]  # deletion\n",
    "                y = lev[i, j-1]  # insertion\n",
    "                z = lev[i-1, j-1]  # substitution\n",
    "                # take the minimum (eg best path/operation)\n",
    "                lev[i, j] = min([x, y, z])\n",
    "                # and if our two current characters don't match, add 1\n",
    "                if a[i] != b[j]:\n",
    "                    # if we have a match, don't add 1\n",
    "                    lev[i, j] += 1\n",
    "    return lev, lev[-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
       "        [ 1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
       "        [ 2.,  1.,  0.,  1.,  2.,  3.,  4.,  5.,  5.,  6.],\n",
       "        [ 3.,  2.,  1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.],\n",
       "        [ 4.,  3.,  1.,  1.,  1.,  2.,  3.,  4.,  4.,  5.],\n",
       "        [ 5.,  4.,  2.,  2.,  2.,  1.,  2.,  3.,  4.,  4.],\n",
       "        [ 6.,  5.,  3.,  3.,  3.,  2.,  1.,  2.,  3.,  4.],\n",
       "        [ 7.,  6.,  4.,  4.,  4.,  3.,  2.,  2.,  3.,  4.],\n",
       "        [ 8.,  7.,  5.,  5.,  5.,  4.,  3.,  2.,  3.,  4.],\n",
       "        [ 9.,  8.,  5.,  6.,  6.,  5.,  4.,  3.,  2.,  3.],\n",
       "        [10.,  9.,  6.,  6.,  6.,  6.,  5.,  4.,  3.,  3.],\n",
       "        [11., 10.,  7.,  7.,  7.,  6.,  6.,  5.,  4.,  3.]]),\n",
       " 3.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leven('Levenshtein', 'Levinsten')\n",
    "\n",
    "# Here we need 3 operations to get from 'Levenshtein' to 'Levinsten', the bottom right value of the matrix is the edit distance (the number of operations needed to get from one string to the other)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "is one of the best known methods for text focused search.\n",
    "\n",
    "To calculate the TF-IDF for a given word (the query) and a sentence (the document), we calculate the **T**erm **F**requency (**TF**), and the **I**nverse **D**ocument **F**requency (**IDF**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we'll merge all docs into a list of lists for easier calculations below\n",
    "docs = [first_sentence, second_sentence]\n",
    "\n",
    "def tfidf(word, sentence):\n",
    "    # term frequency\n",
    "    tf = sentence.count(word) / len(sentence)\n",
    "    # inverse document frequency\n",
    "    idf = np.log10(len(docs) / sum([1 for doc in docs if word in doc]))\n",
    "    return round(tf*idf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence score: 0.0\n",
      "Second sentence score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate the score for each sentence against the word 'football'\n",
    "\n",
    "first = tfidf('football', first_sentence)\n",
    "print(f'First sentence score: {first}')\n",
    "\n",
    "second = tfidf('football', second_sentence)\n",
    "print(f'Second sentence score: {second}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF_IDF vectors is slightly diiferent. WE compute TF-IDF scores for all words withour document vocabulary (all words in all documents) and we produce document speciffic TF_IDF \n",
    "\n",
    "vocab = set(first_sentence + second_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0201, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0201, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0201, 0.0, 0.0, 0.0, 0.0201, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# initialize vectors\n",
    "vec_a = []\n",
    "vec_b = []\n",
    "vec_c = []\n",
    "\n",
    "for word in vocab:\n",
    "    vec_a.append(tfidf(word, first_sentence))\n",
    "    vec_b.append(tfidf(word, second_sentence))\n",
    "\n",
    "print(vec_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0167, 0.0167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0167, 0.0167, 0.0, 0.0167, 0.0, 0.0167, 0.0167, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(vec_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vaccum: {130.68}\n"
     ]
    }
   ],
   "source": [
    "it_data = {\"name\": \"Vaccum\", \"price\": 130.675}\n",
    "\n",
    "print(f\"{it_data['name']}: {{{it_data['price']:.2f}}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered 15.8. It's cool!\n"
     ]
    }
   ],
   "source": [
    "x = float(input(\"Enter today's temperature in Celsius: \"))\n",
    "if x < 10:\n",
    "    print(f\"You entered {x:.1f}. It's cold outside!\")\n",
    "elif 10 < x < 20:\n",
    "    print(f\"You entered {x:.1f}. It's cool!\")\n",
    "else:\n",
    "    print(f\"You entered {x:.1f}. It's hot!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dzhw-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a317a3225f54853d3761a8e99216de1ff0a5ff683c1d575b2a892cb0ed4ad207"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
